{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLFlow():\n",
    "    def __init__(self,df):\n",
    "        self.data = df\n",
    "    \n",
    "    def remove_garbage_values(self, make_negatives_null_columns):\n",
    "        self.data = self.data.replace(['nan'], np.nan)\n",
    "        for col in make_negatives_null_columns:\n",
    "            self.data[col] = self.data[col].apply(lambda x: np.nan if x < 0 else x)\n",
    "            \n",
    "    def generate_columns_yaml(self):\n",
    "        cols_list = self.data.columns.tolist()\n",
    "        cols_dict = {}\n",
    "        for col in cols_list:\n",
    "            five_values = str(self.data[col].sample(5).tolist())\n",
    "            dtype = str(self.data[col].dtype)\n",
    "            try:\n",
    "                fill_rate = str(int(self.data[col].isnull().value_counts(normalize=True)[False]*100)) + ' %'\n",
    "            except:\n",
    "                fill_rate = '0 %'\n",
    "            cols_dict[col] = {'5_sample_values':five_values, 'fill_rate': fill_rate, 'orig_dtype':dtype, 'recast_dtype':dtype, 'tag':'n'}\n",
    "            yaml.dump(cols_dict, open('columns.yaml','w'), default_flow_style=False)\n",
    "            pd.DataFrame(cols_dict).tanspose().to_excel('fill_rate.xlsx')\n",
    "    \n",
    "    def read_columns_yaml(self):\n",
    "        columns_final = yaml.load(open('columns_final.yaml'))\n",
    "        print(pd.DataFrame(columns_final).loc['tag',:].value_counts())\n",
    "        self.categorical_columns = []\n",
    "        self.numerical_columns = []\n",
    "        self.target_column = ''\n",
    "        for column in columns_final:\n",
    "            if(columns_final[column]['tag'] == 'n'):\n",
    "                self.numerical_columns.append(column)\n",
    "            elif(columns_final[column]['tag'] == 'c'):\n",
    "                self.categorical_columns.append(column)\n",
    "            elif(columns_final[column]['tag'] == 't'):\n",
    "                self.target_column = column\n",
    "        for column in columns_final:\n",
    "            if(columns_final[column]['orig_dtype'] != columns_final[column]['recast_dtype']):\n",
    "                try:\n",
    "                    self.data[column] = self.data[column].astype(columns_final[column]['recast_dtype'])\n",
    "                    print(column, ' converted from', columns_final[column]['orig_dtype'], 'to', columns_final[column]['recast_dtype'])\n",
    "                except:\n",
    "                    print('ERROR converting ', column)\n",
    "        selected_cols = self.categorical_columns + self.numerical_columns\n",
    "        selected_cols_dict = {}\n",
    "        for col in selected_cols:\n",
    "            selected_cols_dict[col] = 'y'\n",
    "        yaml.dump(selected_cols_dict, open('selected_features.yaml', 'w'), default_flow_style=False)\n",
    "        selected_cols.append(self.target_column)\n",
    "        self.data = self.data.loc[:,selected_cols]\n",
    "        self.data = self.data.replace(['nan'], np.nan)\n",
    "        print('Target Column:' , self.target_column)\n",
    "        print('Categorical Columns:' , self.categorical_columns)\n",
    "        print('Numerical Columns:', self.numerical_columns)\n",
    "\n",
    "    def impute_missing(self):\n",
    "        print('Data Size before dropping missing Target Variable', self.data.shape[0])\n",
    "        self.data = self.data.loc[self.data[self.target_column].notnul(),:]\n",
    "        print('Data Size after dropping missing Target Variable', self.data.shape[0])\n",
    "        impute_values = {}\n",
    "        for col in self.numerical_columns:\n",
    "            try:\n",
    "                impute_values[col] = self.data[col].median()\n",
    "            except: \n",
    "                pass\n",
    "        for col in self.categorical_columns:\n",
    "            try:\n",
    "                impute_values[col] = '<Missing'\n",
    "            except:\n",
    "                pass\n",
    "        self.data = self.data.fillna(impute_values)\n",
    "        \n",
    "    def cap_outliers(self,outlier_cols):\n",
    "        pass\n",
    "    \n",
    "    def print_categorical(self):\n",
    "        for col in self.categorical_columns:\n",
    "            print(self.data[col].value_counts(dropna=False))\n",
    "    \n",
    "    def club_rare(self):\n",
    "        for col in self.categorical_columns:\n",
    "            min_count = 10\n",
    "            s = self.data[col].value_counts()\n",
    "            self.data.loc[self.data[col].isin(s[s<min_count].index.tolist()),col] = 'RARE'\n",
    "            \n",
    "    def de_one_hot_encode(self,s):\n",
    "        for col in self.categorical_columns:\n",
    "            if(cols in s):\n",
    "                return col\n",
    "        return s\n",
    "    \n",
    "    def derive_features(self):\n",
    "        import derived_features\n",
    "        self.data = derived_features.derive_features(self.data)\n",
    "    \n",
    "    def xgb_reg_single(self,params,selected_features_file,segment,filter_col,thresh,selected_derived_features_file):\n",
    "        from sklearn.model_selection import cross_validate\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        from sklearn.metrics import r2_score\n",
    "        from xgboost.sklearn import XGBRegressor\n",
    "        from sklearn.model_selection import cross_val_predict\n",
    "        reg = XGBRegressor(n_estimators=params['n_estimators'], max_depth=params['max_depth'], \\\n",
    "                           shrinkage=params['shrinkage'], colsample_bynode=params['colsample_bynode'],\\\n",
    "                           n_jobs=params['n_jobs'])\n",
    "        \n",
    "        selected_features_dict = yaml.load(open(selected_features_file))\n",
    "        selected_features = []\n",
    "        for k,v in selected_features_dict.items():\n",
    "            if(v == 'y'):\n",
    "                selected_features.apppend(k)\n",
    "        \n",
    "        selected_derived_features_dict = yaml.load(open(selected_dderived_features_file))\n",
    "        selected_derived_features = []\n",
    "        for k,v in selected_derived_features_dict.items():\n",
    "            if(v == 'y'):\n",
    "                selected_features.append(k)\n",
    "        \n",
    "        X = pd.get_dummies(self.data.loc[selected_features])\n",
    "        variables = X.columns.tolist()\n",
    "        print('DATA SIZE: ', X.shape[0])\n",
    "        X = X.values\n",
    "        y = self.data['cur_liab_amt'].values\n",
    "        \n",
    "        scores = cross_validate(reg, X, y, cv=5,\n",
    "                               scoring=('r2', 'neg_mean_squared_error'),\n",
    "                               return_train_score=True)\n",
    "        #print(scores)\n",
    "        print('Train R2 Scores:' , scores['train_r2'])\n",
    "        print('Test R2 Scores:' , scores['test_r2'])\n",
    "        print('Train R2 Scores Mean:' , np.mean(scores['train_r2']))\n",
    "        print('Test R2 Scores Mean:' , np.mean(scores['test_r2']))\n",
    "        print('Train RMSE Scores:' , np.sqrt(-1*scores['train_neg_mean_squared_error']))\n",
    "        print('Test RMSE Scores:' , np.sqrt(-1*scores['test_neg_mean_squared_error']))\n",
    "        print('Train RMSE Scores Mean:', np.sqrt(-1*np.mean(scores['train_neg_mean_squared_error'])))\n",
    "        print('Test RMSE Scores Mean:', np.sqrt(-1*np.mean(scores['test_neg_mean_squared_error'])))\n",
    "        y_pred = cross_val_predict(reg, X, y, cv=5)\n",
    "        \n",
    "        df_train = pd.DataFrame({'actual':y, 'predicted': y_pred})\n",
    "        df_train['actual_deciles'] = pd.qcut(df_train['actual'], 10, labels = (np.arange(10,0,-1)))\n",
    "        df1 = df_train.groupby('actual_deciles').agg(['mean', 'count'])[['actual' , 'predicted']].sort_values('actual_deciles', ascending=False)\n",
    "        df1['count'] = df1[('actual', 'count')]\n",
    "        df1 = df1.drop([('actual', 'count'), ('predicted', 'count')], axis=1)\n",
    "        df1 = df1.reset_index()\n",
    "        df1.columns = df1.columns.droplevel(1)\n",
    "        cols = ['Decile', 'Mean Actual', 'Mean Predicted', '# Obs']\n",
    "        df1.columns = cols\n",
    "        display(df1)\n",
    "        \n",
    "        error = pd.Series((np.abs(y-y_pred)*100/np.abs(y)))\n",
    "        error_dist = {}\n",
    "        for q in [0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1]:\n",
    "            error_dist[q] = error.quantile(q)\n",
    "        display(pd.Series(error_dist).sort_index())\n",
    "        reg.fit(X,y)\n",
    "        features = pd.DataFrame({'Importance':reg.feature_importances_, 'Variable':variables})\n",
    "        \n",
    "        features['Variable'] = features['Variable'].apply(self.de_one_hot_encode)\n",
    "        features = feature.groupby('Variable').sum().reset_index()\n",
    "        features = features.sort_values('Importance', ascending=False)\n",
    "        features = features[['Importance', 'Variable']]\n",
    "        display(features)\n",
    "        features.to_excel('feature_importances.xlsx')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
